---
title: "Assignment 5"
author: "Caolan McDonagh"
date: "2022-11-22"
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(knitr.kable.NA = '')
```





## Question 1

```{r message=TRUE, warning=FALSE}
#Library init
library(tidyverse)
library(dplyr)
library(plyr)
options(rgl.useNULL = TRUE)
library(ggplot2)
library(ggstatsplot)
library(ggpubr)
library(caret)
library(leaps)
library(car)
library(caTools)
library(olsrr)
library(HDCI)
library(psych)
library(ggcorrplot)
library(glmnet)

#Read data
clothing = read.csv('clothing.csv')

head(clothing)
summary(clothing)

clothing <- clothing %>%
filter(! Amount %in% c(0,1506000))

## Question 1


#Lets check relationship between variables
#Below shows all pair-wise correlations between features on a plot.
plot(clothing)

#More informative view below,
pairs.panels(clothing)
#a scatter plot of matrices, showing histograms on the diagonal line, Pearson correlation above those and bivariate scatter plots below.

#The Pearson correlation coefficient (r) ranges -1 to +1, the higher numbers indicating stronger correlation. This is great for getting a broad look
#at our features. This is ideal here as there isn't too many features to investigate.


#We see slight correlations, especially between the dollar/frequency features of 
#12 and 24 months. Card looks to be pretty useless.

correlation <- round(cor(clothing), 5)
correlation


ggcorrplot(correlation, hc.order = TRUE, type = "lower", lab = TRUE, 
           lab_size = 3, method="circle", colors = c("blue", "white", "red"), 
           outline.color = "gray", show.legend = TRUE, show.diag = FALSE, 
           title="Correlogram of clothing variables")



#The above correlogram shows the correlation between our features. Dollar12 being the most significant and 


# lm(formula = Amount ~ Recency + Freq12 + Dollar12 + Freq24 + Dollar24 + Card, data = clothing)

#Regressor with all original variables
lm1 <- lm(formula = Amount ~ ., data = clothing)

summary(lm1)


#Above is a multiple regression model using all features. We want to test this for multicollinearity, where highly correlated
# variables can mirror changes in one another. This produces unstable results and lots of variance with any changes in the data/model.
#This would in turn lead to over fitting, which we test for later.

corr=cor(clothing)
ggcorrplot(corr, method="circle")

#Using variance inflation factor (VIF) we can check for a numerical measure of multicollinearity.
#Generally, a cutoff of 5 is accepted in VIF, as per below, all values are within this cutoff.
#With all of this I will rule out multicollinearity in our features.
vif(lm1)

#Next we want to look for overfitting when using all of our features.
#To prevent overfitting we can use repeated k-fold cross validation

#split = sample.split(clothing, SplitRatio = 0.7) 
#train <- subset(clothing, split == "TRUE") 
#test <- subset(clothing, split == "FALSE")

train_control <- trainControl(method = "repeatedcv",
                            number = 10, repeats = 3)

model <- train(Amount ~., data = clothing,
               method = "lm",
               trControl = train_control)

print(model)
 
#We want to take the best subset selection to improve the accuracy of the regression model by removing useless/obstructive predictors. This has the benefit of creating a more simple model for easier interpretation.

regfit= regsubsets(Amount ~ ., data = clothing, nvmax = 6)

best <- summary(regfit)

#12 month frequency and cost are the best predictors, followed by recency.


Best <- ols_step_best_subset(lm1)
plot(Best)

allPossible <- ols_step_all_possible(lm1)
plot(allPossible)

stepForward <- ols_step_forward_p(lm1)
plot(stepForward)

stepWise <- ols_step_both_p(lm1)
plot(stepWise)

stepBack <- ols_step_backward_p(lm1)
plot(stepBack)


#Subset selection models above show 

par(mfrow=c(1,2))
plot(best$cp)
plot(regfit,scale = "Cp")

par(mfrow=c(1,2))
plot(best$bic)
plot(regfit,scale = "bic")


par(mfrow=c(1,2))
plot(best$adjr2)
plot(regfit,scale = "adjr2")



#obviously features like freq12 and dollar12 have strong relationships and are directly related. It would be ideal then to use these
#to create new explanatory variables and quadratic terms.

#Allow curvature within the model.
clothing <- clothing %>% mutate("quad12" = Dollar12 * Dollar12)

#We want to predict next purchase value, averaging our 12month purchases brings this to be closer to the next purchase estimates.
clothing <- clothing %>% mutate("spent12months" = Dollar12 / Freq12)

clothing <- clothing %>% mutate("recent12months" = Dollar12 * Recency)

#Remove values that return a NAN, e.g 0 in freq12 - cannot divide by 0

clothing <- na.omit(clothing)

#Regresor with all variables, including new.
lm2 <- lm(formula = Amount ~ ., data = clothing)

summary(lm2)

#Final Regression model with new subset.
finalModel <- lm(formula = Amount ~ spent12months + recent12months + quad12 , data = clothing)
summary(finalModel)


avPlots(finalModel)
#The above avplot (added variable plot) shows a plot for each of our features in the regression model
# x = predictor variable
# y = response variable
# line is the association between the two where the other predictors are constant.The angle of this line matches the estimated coefficients for the predictor (e.g + or -)
# labelled points are the observations with largest residuals and largest partial leverage.

#Regression diagnostics
par(mfrow = c(2, 2))
plot(lm1)

par(mfrow = c(2, 2))
plot(finalModel)



 ## Question 2

#Seed set
set.seed(21249929) 


#Test Train split, 70:30% ratio
train = sample(1:nrow(clothing),round(nrow(clothing), round(nrow(clothing)*0.7)))

#x <- data.matrix(clothing[, c('Recency', 'Freq12','Dollar12','Freq24','Dollar24','Card')])
#x <- data.matrix(clothing[, c('spent12months','recent12months', 'quad12')])
x <- data.matrix(clothing[, c('Recency', 'Freq12','Dollar12','Freq24','Dollar24','Card','spent12months','recent12months', 'quad12')])
y <- clothing$Amount


x.train = x[train,]
y.train = y[train]

x.test = x[-train,]
y.test = y[-train] 

#train model


boot <- bootLasso(x.train,y.train,B=10000,alpha=0.05,cv.method="cv1se",)


#BootLasso coefficients
boot$interval

#Normal Model with all features
confint(lm2, level=0.95)

## Question 3

# q3 requires first row, can be entered manually if needs be with below commented line.
#q3 <- data.frame(Freq12=c(3), Dollar12=c(140),Freq24=c(4),Dollar24=c(225),Card=c(0),spent12months =c(46.66667),recent12months=c(840),quad12=c(19600))
q3 <- clothing[1,]
prediction <- predict(finalModel, newdata=q3)

#Prediction for first customer - 31.64
prediction

predict(finalModel, newdata = q3, interval = 'confidence')
#Above uses confidence interval, lower of 22~ and upper of 40~
#This indicates the average amount is between 22 and 40.


#Checking all observed vs predicted values.

ggplot(clothing, aes(x=predict(finalModel, newdata=clothing), y= Amount)) +
  geom_point() +
  geom_abline(intercept=0, slope=1) +
  labs(x='Predicted Values', y='Actual Values', title='Predicted vs. Actual Values')

#Residual histogram
ggplot(data = clothing[, c('Recency', 'Freq12','Dollar12','Freq24','Dollar24','Card')], aes(x = finalModel$residuals)) +
    geom_histogram(fill = 'steelblue', color = 'black') +
    labs(title = 'Histogram of Residuals', x = 'Residuals', y = 'Frequency')




```